Adversarial Generation of Natural Language


Sai Rajeswar♠∗ Sandeep Subramanian♠∗ Francis Dutil♠ Christopher Pal ♣♠   Aaron Courville♠†
♠MILA, Universite´  de Montre´al	♣E´ cole Polytechnique de Montre´al   †CIFAR Fellow
{sai.rajeswar.mudumba,sandeep.subramanian.1,aaron.courville}@umontreal.ca,
frdutil@gmail.com, christopher.pal@polymtl.ca




 
Abstract

Generative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding im- pressive results for image generation. Ad- vances in the adversarial generation of nat- ural language from noise however are not commensurate with the progress made in generating images, and still lag far be- hind likelihood based methods. In this paper, we take a step towards generating natural language with a GAN objective alone. We introduce a simple baseline that addresses the discrete output space prob- lem without relying on gradient estima- tors and show that it is able to achieve state-of-the-art results on a Chinese poem generation dataset. We present quantita- tive results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language mod- eling results. A conditional version is also described that can generate sequences con- ditioned on sentence characteristics.

1    Introduction

Deep neural networks have recently enjoyed some success at modeling natural language (Mikolov et al., 2010; Zaremba et al., 2014; Kim et al., 2015). Typically, recurrent and convolutional language models are trained to maximize the likelihood of observing a word or character given the previous observations in the sequence
 
item in the sequence given all previous observa- tions. This corresponds to maximum-likelihood training of these models. However this one-step ahead prediction during training makes the model prone to exposure bias (Ranzato et al., 2015; Ben- gio et al., 2015).  Exposure bias occurs when a model is only trained conditioned on ground- truth contexts and is not exposed to its own er- rors (Wiseman and Rush, 2016). An important consequence to exposure bias is that generated se- quences can degenerate as small errors accumu- late. Many important problems in NLP such as machine translation and abstractive summariza- tion are trained via a maximum-likelihood train- ing objective (Bahdanau et al., 2014; Rush et al., 2015), but require the generation of extended se- quences and are evaluated based on sequence-level metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004).
One possible direction towards incorporating a sequence-level training objective is to use Gener- ative Adversarial Networks (GANs) (Goodfellow et al., 2014). While GANs have yielded impres- sive results for modeling images (Radford et al., 2015; Dumoulin et al., 2016), advances in their use for natural language generation has lagged be- hind. Some progress has been made recently in incorporating a GAN objective in sequence mod- eling problems including natural language gen- eration. Lamb et al. (2016) use an adversarial criterion to match the hidden state dynamics of a teacher forced recurrent neural network (RNN) and one that samples from its own output distri- bution across multiple time steps.  Unlike the ap-
 
P (w1 . . . wn) = p(w1) nn
 
proach in Lamb et al. (2016), sequence GANs (Yu
 
These models are commonly trained using a tech-
nique called teacher forcing (Williams and Zipser, 1989) where the inputs to the network are fixed and the model is trained to predict only the next
∗Indicates first authors. Ordering determined by coin flip.
 
et al., 2016) and maximum-likelihood augmented GANs (Che et al., 2017) use an adversarial loss at outputs of an RNN. Using a GAN at the out- puts of an RNN however isn’t trivial since sam- pling from these outputs to feed to the discrimi-
 
nator is a non-differentiable operation. As a re- sult gradients cannot propagate to the generator from the discriminator. Yu et al. (2016) use policy gradient to estimate the generator’s gradient and
 
be shown to minimize the Jensen Shannon Diver- gence (JSD) between the generator’s output distri- bution and the true data distribution.
 
(Che et al., 2017) present an importance sampling
 
min max V (D, G) =	E
 
[log D(x)]
 
based technique. Other alternatives include RE- INFORCE (Williams, 1992), the use of a Gumbel softmax (Jang et al., 2016) and the straighthrough
 
G	D

+	E
z∼P (z)
 
x∼P (x)
[log(1 − D(G(z)))]
 
estimator (Bengio et al., 2013) among others.
In this work, we address the discrete output space problem by simply forcing the discriminator to operate on continuous valued output distribu- tions. The discriminator sees a sequence of proba- bilities over every token in the vocabulary from the generator and a sequence of 1-hot vectors from the true data distribution as in Fig. 1. This technique is identical to that proposed by Gulrajani et al. (2017), which is parallel work to this. In this paper we provide a more complete empirical investiga- tion of this approach to applying GANs to discrete output spaces. We present results using recurrent as well as convolutional architectures on three lan- guage modeling datasets of different sizes at the word and character-level. We also present quanti- tative results on generating sentences that adhere to a simple context-free grammar (CFG), and a richer probabilistic context-free grammar (PCFG). We compare our method to previous works that use a GAN objective to generate natural language, on a Chinese poetry generation dataset. In addi- tion, we present a conditional GAN (Mirza and Osindero, 2014) that generates sentences condi- tioned on sentiment and questions.

2	Generative Adversarial Networks

GANs (Goodfellow et al., 2014) are a general framework used in training generative models by formulating the learning process as a two player minimax game as formulated in the equation be- low. A generator network G tries to generate sam- ples that are as close as possible to the true data distribution P (x) of interest from a fixed noise distribution P (z). We will refer to the samples produced by the generator as G(z). A discrimina- tor network is then trained to distinguish between G(z) and samples from the true data distribution P (x) while the generator network is trained us- ing gradient signals sent by the discriminator by
minimizing log(1 − D(G(z))). Goodfellow et al.
(2014) have shown that, with respect to an opti-
mal discriminator,  the minimax formulation can
 
However, in practice, the generator is trained to
maximize log(D(G(z))) instead, since it provides stronger gradients in the early stages of learning (Goodfellow et al., 2014).
GANs have been reported to be notoriously hard to train in practice (Arjovsky and Bottou, 2017) and several techniques have been proposed to alleviate some of the complexities involved in getting them to  work including modified objec- tive functions and regularization (Salimans et al., 2016; Arjovsky et al., 2017; Mao et al., 2016; Gul- rajani et al., 2017). We discuss some of these prob- lems in the following subsection.
Nowozin et al. (2016) show that it is possible to train GANs with a variety of f-divergence mea- sures besides JSD. Wasserstein GANs (WGANs) (Arjovsky et al., 2017) minimize the earth mover’s distance or Wasserstein distance, while Least Squared GANs (LSGANs) (Mao et al., 2016) modifies replaces the log loss with an L2 loss. WGAN-GP (Gulrajani et al., 2017) incorporate a gradient penalty term on the discriminator’s loss in the WGAN objective which acts as a regular- izer. In this work, we will compare some of these objectives in the context of natural language gen- eration.

2.1	Importance of Wasserstein GANs
Arjovsky and Bottou (2017) argue that part of the problem in training regular GANs is that it seeks to minimize the JSD between the G(z) and P (x).  When the generator is trying to op-
timized log(1 − D(G(z))), the gradients that it
receives vanish as the discriminator is trained to
optimality. The authors also show that when trying to optimize the more practical alternative,
−log(D(G(z))),  the  generator  might  not  suffer
from  vanishing  gradients  but  receives  unstable
training signals. It is also important to consider the fact that highly structured data like images and language lie in low-dimensional manifolds (as is evident by studying their principal components). Wassterstein GANs (Arjovsky et al., 2017) over- come some of the problems in regular GAN train-
 

ing by providing a softer metric to compare the distributions lying in low dimensional manifolds. A key contribution of this work was identifying the importance of a lipschitz constraint which is achieved by clamping the weights of the discrim- inator to lie in a fixed interval. The lipschitz constraint and training the discriminator multiple times for every generator gradient update creates a strong learning signal for the generator.
Gulrajani et al. (2017) present an alternative to weight clamping that they call a gradient penalty to enforce lipschitzness since model performance was reported to be highly sensitive to the clamp- ing hyperparameters. They add the following penalty  to  the  discriminator  training  objective  -
(||VG(z)D(G(z))||2 − 1)2.  A potential concern
regarding our strategy to train our discriminator
to distinguish between sequence of 1-hot vectors from the true data distribution and a sequence of probabilities from the generator is that the discrim- inator can easily exploit the sparsity in the 1-hot vectors to reach optimality. However, Wasster- stein distance with a lipschitz constraint / gradi- ent penalty provides good gradients even under an optimal discriminator and so isn’t a problem for us in practice. Even though it is possible to ex- tract some performance from a regular GAN ob- jective with the gradient penalty (as we show in one of our experiments), WGANs still provide bet- ter gradients to the generator since the discrimina- tor doesn’t saturate often.

3	Model architecture
Let z ∼ N (0, I) be the input to our generator network G from which we will attempt to gener- ate natural language. For implementation conve-
nience, the sample z is of shape n × d where n
is the length of sequence and d is a fixed length
dimension of the noise vector at each time step. The generator then transforms z into a sequence of probability distributions over the vocabulary G(z)
of size n×k where k is the size of our true data dis-
tribution’s vocabulary.  The discriminator network
D is provided with fake samples G(z) and sam- ples from the true data distribution P (x).  Sam- ples from the true distribution are provided as a sequence of 1-hot vectors with each vector serv- ing as an indicator of the observed word in the sample. As described in section 2, the discrimi- nator is trained to discriminate between real and fake samples and the generator is trained to fool
 
















Figure 1: Model architecture the discriminator as in Fig. 1.
We investigate recurrent architectures as in (Lamb et al., 2016; Yu et al., 2016; Che et al., 2017) and convolutional architectures in both the generator as well as the discriminator. The follow- ing subsections detail our architectures.

3.1	Recurrent Models

Recurrent Neural Networks  (RNNs),  particu- larly Long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Networks (Cho et al., 2014) are power- ful models that have been successful at modeling sequential data (Graves and Schmidhuber, 2009; Mikolov et al., 2010). They transform a sequence of input vectors x = x1 . . . xn  into a sequence of hidden states h = h1 . . . hn where each hid- den state maintains a summary of the input up un- til then. RNN language models are autoregres- sive in nature since the input to the network at
time t depends on the output at time t − 1. How-
ever, in the context of generating sequences from
noise, the inputs are pre-determined and there is no direct correspondence between the output at
time t − 1 and the input at time t this fundamen-
tally changes the auto-regressiveness of the RNN.
The RNN does however carry forward informa- tion about its output at time t through subsequent time steps via its hidden states h as evident from its recurrent transition function. In order to incor- porate an explicit dependence between subsequent RNN outputs, we add a peephole connection be- tween the output probability distribution yt−1  at
time t − 1 and the hidden state ht at time t as show
in the LSTM equations below. Typical RNN lan-
 
guage models have a shared affine transformation matrix Wout that is shared across time all steps that projects the hidden state vector to a vector of the same size as the target vocabulary to generate a sequence of outputs y = y1 . . . yt. Subsequently a softmax function is applied to each vector to turn it into a probability distribution over the vocabulary.

yt = softmax(Woutht + bout),

During inference, an output is sampled from the softmax distribution and becomes the input at the subsequent time step. While training the inputs are pre-determined. In all of our models, we perform greedy decoding where we always pick argmax yt. When using the LSTM as a discrim- inator we use a simple binary logistic regression layer on the last hidden state hn to determine the probability of the sample being from the genera- tor’s data distribution or from the real data distri- bution.  P (real)  =  σ(Wpredhn   + bpred).
The  LSTM  update  equations  with  an  output
peephole are :

it = σ(Wxixt + Whiht−1 + Wpiyt−1 + bi) ft = σ(Wxf xt + Whf ht−1 + Wpf yt−1 + bf ) ot = σ(Wxoxt + Whoht−1 + Wpoyt−1 + bo)
ct = tanh(Wxcxt + Whcht−1 + Wpcyt−1 + bc) ct = ft ① ct−1 + it ① ct
ht = ot ① tanh(ct),
where σ is the element-wise sigmoid function, ① is the hadamard product, tanh is the element-wise tanh function. W• and b• are learn-able parame-
ters of the model and it, ft, ot and ct constitute the
input, forget, output and cell states of the LSTM respectively.

3.2	Convolutional Models
Convolutional neural networks (CNNs) have also shown promise at modeling sequential data us- ing 1-dimensional convolutions (Dauphin et al., 2016; Zhang et al., 2015). Convolution filters are convolved across time and the input dimensions are treated as channels. In this work, we explore convolutional generators and discriminators with residual connections (He et al., 2016).
Gulrajani et al. (2017) use a convolutional model for both the generator and discriminator. The generator consists of 5 residual blocks with 2 1-D convolutional layers each. A final 1-D con- volution layer transforms the output of the resid-
 
ual blocks into a sequence of un-normalized vec- tors for each element in the input sequence (noise). These vectors are then normalized using the soft- max function. All convolutions are ’same’ con- volutions with a stride of 1 followed by batch- normalization (Ioffe and Szegedy, 2015) and the ReLU (Nair and Hinton, 2010; Glorot et al., 2011) activation function without any pooling so as to preserve the shape of the input. The discrimina- tor architecture is identical to that of the generator with the final output having a single output chan- nel.

3.3	Curriculum Learning

In likelihood based training of generative language models, models are only trained to make one-step ahead predictions and as a result it is possible to train these models on relatively long sequences even in the initial stages of training. However, in our adversarial formulation, our generator is en- couraged to generate entire sequences that match the true data distribution without explicit supervi- sion at each step of the generation process. As a way  to provide  training signals of  incremen- tal difficulty, we use curriculum learning (Bengio et al., 2009) and train our generator to produce se- quences of gradually increasing lengths as training progresses.

4	Experiments & Data

GAN based methods have often been critiqued for lacking a concrete evaluation strategy (Salimans et al., 2016), however recent work (Wu et al., 2016) uses an annealed importance based tech- nique to overcome this problem.
In the context of generating natural language, it is possible to come up with a simpler approach to evaluate compute the likelihoods of generated samples. We synthesize a data generating distri- bution under which we can compute likelihoods in a tractable manner. We propose a simple evalua- tion strategy for evaluating adversarial methods of generating natural language by constructing a data
generating distribution from a CFG or P−CFG.
It is possible to determine if a sample belongs to

the CFG or the probability of a sample under a P−CFG by using a constituency parser that is pro-
vided with all of the productions in a grammar. Yu et al. (2016) also present a simple idea to esti- mate the likelihood of generated samples by using a randomly initialized LSTM as their data gener-
 
ating distribution. While this is a viable strategy to evaluate generative models of language, a ran- domly initialized LSTM provides little visibility into the complexity of the data distribution itself and presents no obvious way to increase its com- plexity. CFGs and PCFGs however, provide ex- plicit control of the complexity via their produc- tions. They can also be learned via grammar in- duction (Brill, 1993) on large treebanks of natural language and so the data generating distribution is not synthetic as in (Yu et al., 2016).
Typical language models are evaluated by mea- suring the likelihood of samples from the  true data distribution under the model. However, with GANs it is impossible to measure likelihoods un- der the model itself and so we measure the like- lihood of the model’s samples under the true data distribution instead.
We divide our experiments into four categories:
•	Generating language that belongs to a toy CFG and an induced PCFG from the Penn Treebank (Marcus et al., 1993).

•	Chinese poetry generation with comparisons to (Yu et al., 2016) and (Che et al., 2017).
•	Generated samples from a dataset consisting of simple English sentences, the 1-billion- word and Penn Treebank datasets.
•	Conditional GANs that generate sentences conditioned on certain sentence attributes such as sentiment and questions.
 
count the number of unique samples; while this as- sumes that all samples are orthogonal it still serves as a proxy measure of the entropy. We compare various generator, discriminator and GAN objec- tives on this problem.

4.2	Penn Treebank PCFG

To construct a more challenging problem than a simple CFG, we use sections 0-21 of the WSJ sub- section of the Penn Treebank to induce a PCFG using simple count statistics of all productions.


count(A → BC) P (A → BC) =	count(A → ∗)

We train our model on all sentences in the treebank and restrict the output vocabulary to the top 2,000 most frequently occurring words. We evaluate our models on this task by measuring the likelihood of a sample using a Viterbi chart parser (Klein and Manning, 2003). While such a measure mostly captures the grammaticality of a sentence, it is still a reasonable proxy of sample quality.

4.3	Chinese Poetry

Zhang and Lapata (2014) present a dataset of Chi- nese poems that were used to evaluate adversarial training methods for natural language in (Yu et al., 2016) and (Che et al., 2017). The dataset consists of 4-line poems with a variable number of charac- ters in each line. We treat each line in a poem as a training example and use lines of length 5 (poem-
5) and  7  (poem-7)  with the  train/validation/test
2
 
4.1	Simple CFG
 
split
 
specified in (Che et al., 2017).	We use
 

We use a simple and publicly available CFG1 that contains 248 productions. We then generate two sets of data from this CFG - one consisting of sam- ples of length 5 and another of length 11. Each set contains 100,000 samples selected at random from the CFG. The first set has a vocabulary of 36 tokens while the second 45 tokens. We eval- uate our models on this task by measuring the fraction of generated samples that satisfy the rules of the grammar and also measure the diversity in our generated samples. We do this by generating 1,280 samples from noise and computing the frac- tion of those that are valid under our grammar us-
 
BLEU-2 and BLEU-3 to measure model perfor-
mance on this task. Since there is no obvious ”tar- get” for each generated sentence, both works re- port corpus-level BLEU measures using the entire test set as the reference.
