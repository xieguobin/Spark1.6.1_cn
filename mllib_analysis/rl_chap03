In this chapter we consider our first learning methods for estimating value functions and discovering optimal policies. Unlike the previous chapter, here we do not as- sume complete knowledge of the environment. Monte Carlo methods require only experience—sample sequences of states, actions, and rewards from actual or simu- lated interaction with an environment. Learning from actual experience is striking because it requires no prior knowledge of the environment’s dynamics, yet can still attain optimal behavior. Learning from simulated experience is also powerful. Al- though a model is required, the model need only generate sample transitions, not the complete probability distributions of all possible transitions that is required for dynamic programming (DP). In surprisingly many cases it is easy to generate expe- rience sampled according to the desired probability distributions, but infeasible to obtain the distributions in explicit form.
Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks. That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected. Only on the completion of an episode are value estimates and policies changed. Monte Carlo methods can thus be incremental in an episode-by- episode sense, but not in a step-by-step (online) sense. The term “Monte Carlo” is often used more broadly for any estimation method whose operation involves a significant random component. Here we use it specifically for methods based on averaging complete returns (as opposed to methods that learn from partial returns, considered in the next chapter).
Monte Carlo methods sample and average returns for each state–action pair much like the bandit methods we explored in Chapter 2 sample and average rewards for each action. The main difference is that now there are multiple states, each acting like a different bandit problem (like an associative-search or contextual bandit) and that the different bandit problems are interrelated. That is, the return after taking an action in one state depends on the actions taken in later states in the same episode. Because all the action selections are undergoing learning, the problem becomes nonstationary from the point of view of the earlier state.


99
 


To handle the nonstationarity, we adapt the idea of general policy iteration (GPI) developed in Chapter 4 for DP. Whereas there we computed value functions from knowledge of the MDP, here we learn value functions from sample returns with the MDP. The value functions and corresponding policies still interact to attain optimality in essentially the same way (GPI). As in the DP chapter, first we consider the prediction problem (the computation of vπ and qπ for a fixed arbitrary policy π) then policy improvement, and, finally, the control problem and its solution by GPI. Each of these ideas taken from DP is extended to the Monte Carlo case in which only sample experience is available.



5.1	Monte Carlo Prediction

We begin by considering Monte Carlo methods for learning the state-value function for a given policy. Recall that the value of a state is the expected return—expected cumulative future discounted reward—starting from that state. An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value. This idea underlies all Monte Carlo methods.
In particular, suppose we wish to estimate vπ (s), the value of a state s under policy π, given a set of episodes obtained by following π and passing through s. Each occurrence of state s in an episode is called a visit to s. Of course, s may be visited multiple times in the same episode; let us call the first time it is visited in an episode the first visit to s. The first-visit MC method estimates vπ (s) as the average of the returns following first visits to s, whereas the every-visit MC method averages the returns following all visits to s. These two Monte Carlo (MC) methods are very similar but have slightly different theoretical properties. First-visit MC has been most widely studied, dating back to the 1940s, and is the one we focus on in this chapter. Every-visit MC extends more naturally to function approximation and eligibility traces, as discussed in Chapters 9 and 12. First-visit MC is shown in procedural form in the box.


First-visit MC policy evaluation (returns V ≈ vπ )

Initialize:
π ← policy to be evaluated
V  ← an arbitrary state-value function
Returns(s) ← an empty list, for all s ∈ S

Repeat forever:
Generate an episode using π
For each state s appearing in the episode:
G ← return following the first occurrence of s
Append G to Returns(s)
V (s) ← average(Returns(s))
 


Both first-visit MC and every-visit MC converge to vπ (s) as the number of visits (or first visits) to s goes to infinity. This is easy to see for the case of first-visit MC. In this case each return is an independent, identically distributed estimate of vπ (s) with finite variance. By the law of large numbers the sequence of averages of these estimates converges to their expected value. Each average is itself an unbiased
estimate, and the standard deviation of its error falls as 1/√n, where n is the number
of returns averaged (i.e., the estimate is said to converge quadratically ). Every-visit MC is less straightforward, but its estimates also converge quadratically to vπ (s) (Singh and Sutton, 1996).
The use of Monte Carlo methods is best illustrated through an example.

Example 5.1: Blackjack The object of the popular casino card game of blackjack is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a natural. He then wins unless the dealer also has a natural, in which case the game is a draw. If the player does not have a natural, then he can request additional cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust ). If he goes bust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome—win, lose, or draw—is determined by whose final sum is closer to 21.

Playing blackjack is naturally formulated as an episodic finite MDP. Each game of blackjack is an episode. Rewards of +1, −1, and 0 are given for winning, losing, and
drawing, respectively. All rewards within a game are zero, and we do not discount (γ = 1); therefore these terminal rewards are also the returns. The player’s actions are to hit or to stick. The states depend on the player’s cards and the dealer’s showing card. We assume that cards are dealt from an infinite deck (i.e., with replacement) so that there is no advantage to keeping track of the cards already dealt. If the player holds an ace that he could count as 11 without going bust, then the ace is said to be usable. In this case it is always counted as 11 because counting it as 1 would make the sum 11 or less, in which case there is no decision to be made because, obviously, the player should always hit. Thus, the player makes decisions on the basis of three variables: his current sum (12–21), the dealer’s one showing card (ace–10), and whether or not he holds a usable ace. This makes for a total of 200 states.
Consider the policy that sticks if the player’s sum is 20 or 21, and otherwise hits. To find the state-value function for this policy by a Monte Carlo approach, one simulates many blackjack games using the policy and averages the returns following each state. Note that in this task the same state never recurs within one episode, so there is no difference between first-visit and every-visit MC methods. In this way, we obtained the estimates of the state-value function shown in Figure 5.1. The estimates for states with a usable ace are less certain and less regular because these
 

 
After 10,000 episodes
 
After 500,000 episodes
 



Usable	+1
ace
!1






No usable ace





Figure 5.1: Approximate state-value functions for the blackjack policy that sticks only on 20 or 21, computed by Monte Carlo policy evaluation.


states are less common. In any event, after 500,000 games the value function is very well  approximated.
Although we have complete knowledge of the environment in this task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events—in particular, they require the quantities
p(s1, r|s, a)—and it is not easy to determine these for blackjack. For example, suppose
the player’s sum is 14 and he chooses to stick.  What is his expected reward as a
function of the dealer’s showing card? All of these expected rewards and transition probabilities must be computed before DP can be applied, and such computations are often complex and error-prone. In contrast, generating the sample games required by Monte Carlo methods is easy. This is the case surprisingly often; the ability of Monte Carlo methods to work with sample episodes alone can be a significant advantage even when one has complete knowledge of the environment’s dynamics.



Can we generalize the idea of backup diagrams to Monte Carlo algorithms? The general idea of a backup diagram is to show at the top the root node to be updated and to show below all the transitions and leaf nodes whose rewards and estimated values contribute to the update. For Monte Carlo estimation of vπ , the root is a state node, and below it is the entire trajectory of transitions along a particular single episode, ending at the terminal state, as in Figure 5.2. Whereas the DP diagram (Figure 3.4-left) shows all possible transitions, the Monte Carlo diagram shows only those sampled on the one episode. Whereas the DP diagram includes only one-step transitions, the Monte Carlo diagram goes all the way to the end of the episode. These differences in the diagrams accurately reflect the fundamental differences between the algorithms.
 


















terminal state


Figure 5.2: The backup diagram for Monte Carlo estimation of vπ .


An important fact about Monte Carlo methods is that the estimates for each state are independent. The estimate for one state does not build upon the estimate of any other state, as is the case in DP. In other words, Monte Carlo methods do not bootstrap as we defined it in the previous chapter.
In particular, note that the computational expense of estimating the value of a single state is independent of the number of states. This can make Monte Carlo methods particularly attractive when one requires the value of only one or a subset of states. One can generate many sample episodes starting from the states of interest, averaging returns from only these states ignoring all others. This is a third advantage Monte Carlo methods can have over DP methods (after the ability to learn from actual experience and from simulated experience).

Example 5.2: Soap Bubble
Suppose a wire frame forming a closed loop is dunked in soapy water to form a soap surface or bubble conforming at its edges to the wire frame. If the geometry of the wire frame is irregular but known, how can you compute the shape of the surface? The shape has the property that the total force on each point exerted by neighboring points is zero (or else the shape would change).   This means that
 
the surface’s height at any point is the aver- age of its heights at points in a small circle
 
A bubble on a wire loop
 
around that point. In addition, the surface must meet at its boundaries with the wire frame. The usual approach to problems of this kind is to put a grid over the area covered by the surface and solve for its height at the grid points by an iterative computation. Grid points at the boundary are forced to the wire frame, and all others are adjusted toward the average of the heights of their four nearest neighbors.
 


This process then iterates, much like DP’s iterative policy evaluation, and ultimately converges to a close approximation to the desired surface.
This is similar to the kind of problem for which Monte Carlo methods were origi- nally designed. Instead of the iterative computation described above, imagine stand- ing on the surface and taking a random walk, stepping randomly from grid point to neighboring grid point, with equal probability, until you reach the boundary. It turns out that the expected value of the height at the boundary is a close approximation to the height of the desired surface at the starting point (in fact, it is exactly the value computed by the iterative method described above). Thus, one can closely approximate the height of the surface at a point by simply averaging the bound- ary heights of many walks started at the point. If one is interested in only the value at one point, or any fixed small set of points, then this Monte Carlo method can be far more efficient than the iterative method based on local consistency.


Exercise 5.1 Consider the diagrams on the right in Figure 5.1. Why does the estimated value function jump up for the last two rows in the rear? Why does it drop off for the whole last row on the left? Why are the frontmost values higher in the upper diagrams than in the lower?


5.2	Monte Carlo Estimation of Action Values

If a model is not available, then it is particularly useful to estimate action values (the values of state–action pairs) rather than state values. With a model, state values alone are sufficient to determine a policy; one simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the chapter on DP. Without a model, however, state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of our primary goals for Monte Carlo methods is to estimate q∗. To achieve this, we first consider the policy evaluation problem for action values.
The policy evaluation problem for action values is to estimate qπ (s, a), the expected return when starting in state s, taking action a, and thereafter following policy π. The Monte Carlo methods for this are essentially the same as just presented for state values, except now we talk about visits to a state–action pair rather than to a state. A state–action pair s, a is said to be visited in an episode if ever the state s is visited and action a is taken in it. The every-visit MC method estimates the value of a state–action pair as the average of the returns that have followed all the visits to it. The first-visit MC method averages the returns following the first time in each episode that the state was visited and the action was selected. These methods converge quadratically, as before, to the true expected values as the number of visits to each state–action pair approaches infinity.
The only complication is that many state–action pairs may never be visited.  If
π is a deterministic policy, then in following π one will observe returns only for
 


one of the actions from each state. With no returns to average, the Monte Carlo estimates of the other actions will not improve with experience. This is a serious problem because the purpose of learning action values is to help in choosing among the actions available in each state. To compare alternatives we need to estimate the value of all the actions from each state, not just the one we currently favor.
This is the general problem of maintaining exploration, as discussed in the context of the k-armed bandit problem in Chapter 2. For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes start in a state–action pair, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state–action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of exploring starts.
The assumption of exploring starts is sometimes useful, but of course it cannot be relied upon in general, particularly when learning directly from actual interaction with an environment. In that case the starting conditions are unlikely to be so helpful. The most common alternative approach to assuring that all state–action pairs are encountered is to consider only policies that are stochastic with a nonzero probability of selecting all actions in each state. We discuss two important variants of this approach in later sections. For now, we retain the assumption of exploring starts and complete the presentation of a full Monte Carlo control method.

Exercise 5.2  What is the backup diagram for Monte Carlo estimation of qπ ?


5.3	Monte Carlo Control

 
We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate optimal poli- cies. The overall idea is to proceed according to the same pattern as in the DP chapter, that is, according to the idea of generalized policy iteration (GPI). In GPI one maintains both an approximate policy and an approximate value func- tion. The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function, as suggested by the diagram to the right. These two
 


evaluation
Q     q⇡

Q
⇡   greedy(Q)

improvement
 
kinds of changes work against each other to some extent, as each creates a mov- ing target for the other, but together they cause both policy and value function to approach  optimality.
To begin, let us consider a Monte Carlo version of classical policy iteration. In this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with an arbitrary policy π0 and ending with the optimal policy and optimal action-value function:

E	I	E	I	E	I	E
π0 −→ qπ0  −→ π1 −→ qπ1  −→ π2 −→ • • • −→ π∗ −→ q∗,
 

 

where
 
−→ denotes a complete policy evaluation and
 
−→ denotes a complete pol-
 
icy improvement.   Policy evaluation is done exactly as described in the preceding
section. Many episodes are experienced, with the approximate action-value func- tion approaching the true function asymptotically. For the moment, let us assume that we do indeed observe an infinite number of episodes and that, in addition, the episodes are generated with exploring starts. Under these assumptions, the Monte Carlo methods will compute each qπk  exactly, for arbitrary πk .
Policy improvement is done by making the policy greedy with respect to the current
value function. In this case we have an action-value function, and therefore no model is needed to construct the greedy policy. For any action-value function q, the
corresponding greedy policy is the one that, for each s ∈ S, deterministically chooses
an action with maximal action-value:
 
π(s) .
 

arg max q(s, a).	(5.1)
a
 

Policy improvement then can be done by constructing each πk+1 as the greedy policy with respect to qπk . The policy improvement theorem (Section 4.2) then applies to
πk  and πk+1  because, for all s ∈ S,

qπk (s, πk+1(s))  =  qπk (s, arg max qπk (s, a))
a
= max qπk (s, a)
a
≥  qπk (s, πk (s))
≥  vπk (s).

As we discussed in the previous chapter, the theorem assures us that each πk+1 is uniformly better than πk , or just as good as πk , in which case they are both optimal policies. This in turn assures us that the overall process converges to the optimal policy and optimal value function. In this way Monte Carlo methods can be used to find optimal policies given only sample episodes and no other knowledge of the environment’s  dynamics.
We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for the Monte Carlo method. One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes. To obtain a practical algorithm we will have to remove both assumptions. We postpone consideration of the first assumption until later in this chapter.
For now we focus on the assumption that policy evaluation operates on an infinite number of episodes. This assumption is relatively easy to remove. In fact, the same issue arises even in classical DP methods such as iterative policy evaluation, which also converge only asymptotically to the true value function. In both DP and Monte Carlo cases there are two ways to solve the problem. One is to hold firm to the idea of approximating qπk in each policy evaluation. Measurements and assumptions are made to obtain bounds on the magnitude and probability of error in the estimates, and then sufficient steps are taken during each policy evaluation to assure that these bounds are sufficiently small. This approach can probably be made
 


completely satisfactory in the sense of guaranteeing correct convergence up to some level of approximation. However, it is also likely to require far too many episodes to be useful in practice on any but the smallest problems.
The second approach to avoiding the infinite number of episodes nominally required for policy evaluation is to forgo trying to complete policy evaluation before returning to policy improvement. On each evaluation step we move the value function toward qπk , but we do not expect to actually get close except over many steps. We used this idea when we first introduced the idea of GPI in Section 4.6. One extreme form of the idea is value iteration, in which only one iteration of iterative policy evaluation is performed between each step of policy improvement. The in-place version of value iteration is even more extreme; there we alternate between improvement and evaluation steps for single states.
For Monte Carlo policy evaluation  it is natural to alternate between evaluation and improvement on an episode-by-episode basis. After each episode, the observed returns are used for policy evaluation, and then the policy is improved at all the states visited in the episode. A complete simple algorithm along these lines, which we call Monte Carlo ES, for Monte Carlo with Exploring Starts, is given in the box.
In Monte Carlo ES, all the returns for each state–action pair are accumulated and averaged, irrespective of what policy was in force when they were observed. It is easy to see that Monte Carlo ES cannot converge to any suboptimal policy. If it did, then the value function would eventually converge to the value function for that policy, and that in turn would cause the policy to change. Stability is achieved only when both the policy and the value function are optimal. Convergence to this optimal fixed point seems inevitable as the changes to the action-value function decrease over time, but has not yet been formally proved. In our opinion, this is one of the most fundamental open theoretical questions in reinforcement learning (for a partial solution, see Tsitsiklis, 2002).



Monte Carlo ES (Exploring Starts)
Initialize, for all s ∈ S, a ∈ A(s):
Q(s, a) ← arbitrary
π(s) ← arbitrary
Returns(s, a) ← empty list

Repeat forever:
Choose S0 ∈ S and A0 ∈ A(S0) s.t. all pairs have probability > 0
Generate an episode starting from S0, A0, following π
For each pair s, a appearing in the episode:
G ← return following the first occurrence of s, a
Append G to Returns(s, a)
Q(s, a) ← average(Returns(s, a))
For each s in the episode:
π(s) ← arg maxa Q(s, a)
 


 





Usable ace







No usable ace
 
!*	v*
21
20
19
18
17
16
15
14
13
12
11
A 2 3 4 5 6 7 8 9 10

21
20
19
17	+1
16
15	"1
14
13
12
11
A 2 3 4 5 6 7 8 9 10
Dealer showing
 

Figure 5.3: The optimal policy and state-value  function  for  blackjack,  found  by  Monte Carlo ES (Figure 5.4). The state-value function shown was computed from the action-value function found by Monte Carlo ES.


Example 5.3: Solving Blackjack It is straightforward to apply Monte Carlo ES to blackjack. Since the episodes are all simulated games, it is easy to arrange for exploring starts that include all possibilities. In this case one simply picks the dealer’s cards, the player’s sum, and whether or not the player has a usable ace, all at random with equal probability. As the initial policy we use the policy evaluated in the previous blackjack example, that which sticks only on 20 or 21. The initial action-value function can be zero for all state–action pairs. Figure 5.3 shows the optimal policy for blackjack found by Monte Carlo ES. This policy is the same as the “basic” strategy of Thorp (1966) with the sole exception of the leftmost notch in the policy for a usable ace, which is not present in Thorp’s strategy. We are uncertain of the reason for this discrepancy, but confident that what is shown here is indeed the optimal policy for the version of blackjack we have described.


5.4	Monte Carlo Control without Exploring Starts

How can we avoid the unlikely assumption of exploring starts? The only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches to ensuring this, resulting in what we call on-policy methods and off-policy methods.  On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas off-policy methods evaluate or improve a policy different from that used to generate the data. The Monte Carlo ES method developed above is an example of an on-policy method. In this section we show how an on-policy Monte Carlo control method can be designed that does not use the unrealistic assumption of exploring starts. Off-policy methods
 
5.4.	MONTE   CARLO   CONTROL   WITHOUT   EXPLORING   STARTS 	109


are considered in the next section.
In on-policy control methods the policy is generally soft, meaning that π(a|s) > 0 for all s ∈ S and all a ∈ A(s), but gradually shifted closer and closer to a deterministic
optimal policy. Many of the methods discussed in Chapter 2 provide mechanisms for this. The on-policy method we present in this section uses ε-greedy policies, meaning that most of the time they choose an action that has maximal estimated action value, but with probability ε they instead select an action at random.  That
 
is, all nongreedy actions are given the minimal probability of selection,	ε
|A(s)|
 
, and
 
the remaining bulk of the probability, 1	ε +	ε
|A(s)|
 
, is given to the greedy action.
 
The ε-greedy policies are examples of ε-soft policies, defined as policies for which
 
π(a s)	ε
|A(s)|
 
for all states and actions, for some ε > 0.  Among ε-soft policies,
 
ε-greedy policies are in some sense those that are closest to greedy.
The overall idea of on-policy Monte Carlo control is still that of GPI. As in Monte Carlo ES, we use first-visit MC methods to estimate the action-value function for the current policy. Without the assumption of exploring starts, however, we cannot sim- ply improve the policy by making it greedy with respect to the current value function, because that would prevent further exploration of nongreedy actions. Fortunately, GPI does not require that the policy be taken all the way to a greedy policy, only that it be moved toward a greedy policy. In our on-policy method we will move it only to an ε-greedy policy. For any ε-soft policy, π, any ε-greedy policy with respect to qπ is guaranteed to be better than or equal to π. The complete algorithm is given in the box below.

That any ε-greedy policy with respect to qπ is an improvement over any ε-soft policy π is assured by the policy improvement theorem.   Let π1  be the ε-greedy
policy.  The conditions of the policy improvement theorem apply because for any




On-policy first-visit MC control (for ε-soft policies)

Initialize, for all s ∈ S, a ∈ A(s): Q(s, a) ← arbitrary Returns(s, a) ← empty list
π(a|s) ← an arbitrary ε-soft policy

Repeat forever:
(a)	Generate an episode using π
(b)	For each pair s, a appearing in the episode:
G ← return following the first occurrence of s, a
Append G to Returns(s, a)
Q(s, a) ← average(Returns(s, a))
(c)	For each s in the episode: A∗ ← arg maxa Q(s, a) For all a ∈ A(s):
( 1 − ε + ε/|A(s)|  if a = A∗
 
π(a|s) ←
 
ε/|A(s)|	if a /= A∗
 


s ∈ S:
qπ (s, π1(s))  =  ... π1(a|s)qπ (s, a)
a
 
E
=
|A(s)|
 
...

a
 

qπ (s, a)  + (1	ε) max qπ (s, a)	(5.2)
a
 
E	...
≥
 
qπ (s, a) + (1 − ε)
 
... π(a|s) −
 
ε
|A(s)| q  (s, a)
 
|A(s)|  a
 
a	1 − ε
 
(the sum is a weighted average with nonnegative weights summing to 1, and as such it must be less than or equal to the largest number averaged)
 
E
=
|A(s)|
 
...

a
 
qπ (s, a) −
 
E
|A(s)|
 
...

a
 

qπ (s, a) +
 
...

a
 
π(a|s)qπ (s, a)
 
= vπ (s).
Thus, by the policy improvement theorem, π1 ≥ π (i.e., vπI (s) ≥ vπ (s), for all s ∈ S). We now prove that equality can hold only when both π1 and π are optimal among the
ε-soft policies, that is, when they are better than or equal to all other ε-soft policies.
Consider a new environment that is just like the original environment, except with the requirement that policies be ε-soft “moved inside” the environment. The new environment has the same action and state set as the original and behaves as follows.
If in state s and taking action a, then with probability 1 − ε the new environment
behaves exactly like the old environment.  With probability ε it repicks the action
at random, with equal probabilities, and then behaves like the old environment with the new, random action. The best one can do in this new environment with general policies is the same as the best one could do in the original environment with ε-soft v∗ and q∗ denote the optimal value functions for the new environment.
Then a policy π is optimal among ε-soft policies if and only if vπ = v∗.  From the
definition of v  we know that it is the unique solution to
 
 
E
q (s, a) +
a	|A(s)|
 
... q (s, a)
 ∗
a
 
=   (1 − ε) max ... p(s1, r|s, a) r + γv (s1) 
 
a
sI,r
E	... ...
 
 ∗

 	v (s1) .
 
+
|A(s)|  a
 

sI,r
 
p(s1, r|s, a)
 
r + γ ∗
 
When equality holds and the ε-soft policy π is no longer improved, then we also know, from (5.2), that
 

vπ (s)   =  (1	ε) max qπ (s, a) +
a
 
E
|A(s)|
 
...

a
 

qπ (s, a)
 
=   (1 − ε) max ... p(s1, r|s, a) r + γvπ (s1) 
 
sI,r
+	E	... ...
 

p(s1, r|s, a)
 

r + γvπ (s1) .
 
|A(s)|  a
 
sI,r
 


However, this equation is the same as the previous one, except for the substitution
v∗ is the unique solution, it must be that vπ = v∗.
In essence, we have shown in the last few pages that policy iteration works for ε-soft policies. Using the natural notion of greedy policy for ε-soft policies, one is assured of improvement on every step, except when the best policy has been found among the ε-soft policies. This analysis is independent of how the action-value functions are determined at each stage, but it does assume that they are computed exactly. This brings us to roughly the same point as in the previous section. Now we only achieve the best policy among the ε-soft policies, but on the other hand, we have eliminated the assumption of exploring starts.



5.5	Off-policy Prediction via Importance Sampling


All learning control methods face a dilemma: They seek to learn action values con- ditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? The on-policy approach in the preceding section is actually a compromise—it learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to use two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to gen- erate behavior. The policy being learned about is called the target policy, and the policy used to generate behavior is called the behavior policy. In this case we say that learning is from data “off” the target policy, and the overall process is termed off-policy  learning.
Throughout the rest of this book we consider both on-policy and off-policy meth- ods. On-policy methods are generally simpler and are considered first. Off-policy methods require additional concepts and notation, and because the data is due to a different policy, off-policy methods are often of greater variance and are slower to converge. On the other hand, off-policy methods are more powerful and general. They include on-policy methods as the special case in which the target and behavior policies are the same. Off-policy methods also have a variety of additional uses in applications. For example, they can often be applied to learn from data generated by a conventional non-learning controller, or from a human expert. Off-policy learning is also seen by some as key to learning multi-step predictive models of the world’s dynamics (Sutton, 2009, Sutton et al., 2011).
In this section we begin the study of off-policy methods by considering the predic- tion problem, in which both target and behavior policies are fixed. That is, suppose we wish to estimate vπ or qπ , but all we have are episodes following another policy
µ, where µ /= π.  In this case, π is the target policy, µ is the behavior policy, and
both policies are considered fixed and given.
In order to use episodes from µ to estimate values for π, we require that every action taken under π is also taken, at least occasionally, under µ. That is, we require
 


that π(a|s) > 0 implies µ(a|s) > 0.  This is called the assumption of coverage.  It
follows from coverage that µ must be stochastic in states where it is not identical
to π. The target policy π, on the other hand, may be deterministic, and, in fact, this is a case of particular interest in control problems. In control, the target policy is typically the deterministic greedy policy with respect to the current action-value function estimate. This policy becomes a deterministic optimal policy while the behavior policy remains stochastic and more exploratory, for example, an ε-greedy policy. In this section, however, we consider the prediction problem, in which π is unchanging and given.
Almost all off-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio. Given a starting state St, the prob- ability of the subsequent state–action trajectory, At, St+1, At+1, . . . , ST , occurring under any policy π is
T −1
n π(Ak |Sk )p(Sk+1|Sk , Ak ),
k=t
where p here is  the  state-transition  probability  function  defined  by  (3.8).  Thus, the relative probability of the trajectory under the target and behavior policies (the importance-sampling  ratio)  is
 

ρT  .
 
k=t π(Ak |Sk )p(Sk+1|Sk , Ak )
 
T   1
n π(Ak |Sk )
 
t  =	=
k=t µ(Ak |Sk )p(Sk+1|Sk , Ak )
 

k=t
 

µ(Ak
 
|Sk
 
.	(5.3)
)
 
Note that although the trajectory probabilities depend on the MDP’s transition probabilities, which are generally unknown, all the transition probabilities cancel. The importance sampling ratio ends up depending only on the two policies and not at all on the MDP.
Now we are ready to give a Monte Carlo algorithm that uses a batch of observed episodes following policy µ to estimate vπ (s). It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time t = 101. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state s is visited, denoted T(s). This is for an every-visit method; for a first-visit method, T(s) would only include time steps that were first visits to s within their episodes. Also, let T (t) denote the first time of termination following time t, and Gt denote the
return after t up through T (t). Then {Gt}t∈T(s) are the returns that pertain to state
s, and {ρT (t)}        are the corresponding importance-sampling ratios.  To estimate
vπ (s), we simply scale the returns by the ratios and average the results:
 

V (s) .
 
>-	T (t)
t∈T(s)   t	t
|T(s)|
 


.	(5.4)
 


When importance sampling is done as a simple average in this way it is called ordinary importance sampling.
An important alternative is weighted importance sampling, which uses a weighted
average, defined as

 

V (s) .
 
>-	T (t)
t∈T(s)   t	t
 


,	(5.5)
 
>-	T (t)
t∈T(s)   t

or zero if the denominator is zero. To understand these two varieties of importance sampling, consider their estimates after observing a single return. In the weighted-
average estimate, the ratio ρT (t) for the single return cancels in the numerator and
denominator, so that the estimate is equal to the observed return independent of the ratio (assuming the ratio is nonzero). Given that this return was the only one observed, this is a reasonable estimate, but of course its expectation is vµ(s) rather than vπ (s), and in this statistical sense it is biased. In contrast, the simple average (5.4) is always vπ (s) in expectation (it is unbiased), but it can be extreme. Suppose the ratio were ten, indicating that the trajectory observed is ten times as likely under the target policy as under the behavior policy. In this case the ordinary importance- sampling estimate would be ten times the observed return. That is, it would be quite far from the observed return even though the episode’s trajectory is considered very representative of the target policy.
Formally, the difference between the two kinds of importance sampling is expressed in their biases and variances. The ordinary importance-sampling estimator is unbi- ased whereas the weighted importance-sampling estimator is biased (the bias con- verges asymptotically to zero). On the other hand, the variance of the ordinary importance-sampling estimator is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite (Precup, Sutton, and Dasgupta 2001). In practice, the weighted estimator usually has dramatically lower variance and is strongly pre- ferred. Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximation that we explore in the second part of this book.
A complete every-visit MC algorithm for off-policy policy evaluation using weighted importance sampling is given in the next section on page 117.

Example 5.4: Off-policy Estimation of a Blackjack State Value
We applied both ordinary and weighted importance-sampling methods to estimate the value of a single blackjack state from off-policy data. Recall that one of the advantages of Monte Carlo methods is that they can be used to evaluate a single state without forming estimates for any other states. In this example, we evaluated the state in which the dealer is showing a deuce, the sum of the player’s cards is 13, and the player has a usable ace (that is, the player holds an ace and a deuce, or equivalently three aces).  The data was generated by starting in this state then
 


4


 

Mean square error	2
(average over 100 runs)



0
 
Ordinary importance sampling





Weighted importance sampling


0	10	100	1000	10,000
Episodes (log scale)
 

Figure 5.4: Weighted importance sampling produces lower error estimates of the value of a single blackjack state from off-policy episodes (see Example 5.4).



choosing to hit or stick at random with equal probability (the behavior policy). The target policy was to stick only on a sum of 20 or 21, as in Example 5.1. The value
of this state under the target policy is approximately −0.27726 (this was determined
by separately generating one-hundred million episodes using the target policy and
averaging their returns). Both off-policy methods closely approximated this value after 1000 off-policy episodes using the random policy. To make sure they did this reliably, we performed 100 independent runs, each starting from estimates of zero and learning for 10,000 episodes. Figure 5.4 shows the resultant learning curves—the squared error of the estimates of each method as a function of number of episodes, averaged over the 100 runs. The error approaches zero for both algorithms, but the weighted importance-sampling method has much lower error at the beginning, as is typical  in  practice.

Example 5.5: Infinite Variance
The estimates of ordinary importance sampling will typically have infinite variance, and thus unsatisfactory convergence properties, whenever the scaled returns have infinite variance—and this can easily happen in off-policy learning when trajecto- ries contain loops. A simple example is shown inset in Figure 5.5. There is only one nonterminal state s and two actions, end and back. The end action causes a deterministic transition to termination, whereas the back action transitions, with probability 0.9, back to s or, with probability 0.1, on to termination. The rewards are +1 on the latter transition and otherwise zero. Consider the target policy that always selects back. All episodes under this policy consist of some number (possibly zero) of transitions back to s followed by termination with a reward and return of
+1. Thus the value of s under the target policy is 1. Suppose we are estimating this value from off-policy data using the behavior policy that selects end and back with equal  probability.
The lower part of Figure 5.5 shows ten independent runs of the first-visit MC algo-
 



 



back
 



0.1

0.9
 
R = +1
 




end
 

⇡(back|s)= 1

1
µ(back|s) = 2
 

2
Monte-Carlo estimate of v⇡ (s) with
ordinary
importance    1 sampling  (ten runs)


 
0
1	10	100	1000	10,000
 

100,000	1,000,000     10,000,000    100,000,000
 
Episodes (log scale)

Figure 5.5: Ordinary importance sampling produces surprisingly unstable estimates on the one-state MDP shown inset (Example 5.5). The correct estimate here is 1, and, even though this is the expected value of a sample return (after importance sampling), the variance of the samples is infinite, and the estimates do not convergence to this value. These results are for off-policy first-visit MC.


rithm using ordinary importance sampling. Even after millions of episodes, the esti- mates fail to converge to the correct value of 1. In contrast, the weighted importance- sampling algorithm would give an estimate of exactly 1 everafter the first episode that ended with the back action. All returns not equal to 1 (that is, ending with the end action) would be inconsistent with the target policy and thus would have
t of zero and contribute neither to the numerator nor denominator of  (5.5). The weighted importance-sampling algorithm produces a weighted average of only the returns consistent with the target policy, and all of these would be exactly 1.
We can verify that the variance of the importance-sampling-scaled returns is infi- nite in this example by a simple calculation. The variance of any random variable X is the expected value of the deviation from its mean X¯ , which can be written
 
Var[X]  .
 
E rX − X¯ )2   = ErX2 − 2XX¯ + X¯ 2l = ErX2l − X¯ 2.
 

Thus, if the mean is finite, as it is in our case, the variance is infinite if and only if the expectation of the square of the random variable is infinite. Thus, we need only show that the expected square of the importance-sampling-scaled return is infinite:
 
(T −1 π(A  S )
 
\2
 
E	n
t=0
 
t| t  G0	 .
µ(At|St)
 

To compute this expectation, we break it down into cases based on episode length and termination.  First note that, for any episode ending with the end action, the
 


importance sampling ratio is zero, because the target policy would never take this action; these episodes thus contribute nothing to the expectation (the quantity in parenthesis will be zero) and can be ignored. We need only consider episodes that involve some number (possibly zero) of back actions that transition back to the nonterminal state, followed by a back action transitioning to termination. All of these episodes have a return of 1, so the G0 factor can be ignored. To get the expected square we need only consider each length of episode, multiplying the probability of the episode’s occurrence by the square of its importance-sampling ratio, and add these up:
 

1
= 2 • 0.1
1
 
a 1 12
0.5
1	a 1
 



1 12
 


(the length 1 episode)
 
+ 2 • 0.9 •
 
2 • 0.1
 

0.5 0.5
 
(the length 2 episode)
 
1
+	• 0.9 •
 
1
• 0.9 •
 
1	a 1	1
• 0.1
 
1 12
 

(the length 3 episode)
 
2	2	2
+ • • •
∞
 
0.5 0.5 0.5
 
= 0.1 ... 0.9k • 2k • 2
k=0
∞
= 0.2 ... 1.8k
k=0
= ∞.


Exercise 5.3 What is the equation analogous to (5.5) for action values Q(s, a) instead of state values V (s), again given returns generated using µ?

Exercise 5.4 In learning curves such as those shown in Figure 5.4 error generally decreases with training, as indeed happened for the ordinary importance-sampling method. But for the weighted importance-sampling method error first increased and then decreased. Why do you think this happened?

Exercise 5.5 The results with Example 5.5 and shown in Figure 5.5 used a first- visit MC method. Suppose that instead an every-visit MC method was used on the same problem. Would the variance of the estimator still be infinite? Why or why not?


5.6	Incremental  Implementation

Monte Carlo prediction methods can be implemented incrementally, on an episode- by-episode basis, using extensions of the techniques described in Chapter 2 (Sec- tion 2.3). Whereas in Chapter 2 we averaged rewards, in Monte Carlo methods we average returns. In all other respects exactly the same methods as used in Chapter
 
5.6.	INCREMENTAL                                                     IMPLEMENTATION 	117


2 can be used for on-policy Monte Carlo methods. For off-policy Monte Carlo meth- ods, we need to separately consider those that use ordinary importance sampling and those that use weighted importance sampling.

In ordinary importance sampling, the returns are scaled by the importance sam- pling ratio ρT (t)  (5.3), then simply averaged.  For these methods we can again use
the incremental methods of Chapter 2, but using the scaled returns in place of the rewards of that chapter. This leaves the case of off-policy methods using weighted importance sampling. Here we have to form a weighted average of the returns, and a slightly different incremental algorithm is required.

Suppose we have a sequence of returns G1, G2, . . . , Gn−1, all starting in the same state and each with a corresponding random weight Wi (e.g., Wi = ρT (t)). We wish
to form the estimate
>-n−1 Wk Gk
 
Vn  .
 
k=1
>-n−1
k=1
 
,	n ≥ 2,	(5.6)
 

and keep it up-to-date as we obtain a single additional return Gn. In addition to keeping track of Vn, we must maintain for each state the cumulative sum Cn of the weights given to the first n returns. The update rule for Vn is

Wn
 



and
 
Vn+1  .	n +
Cn
 
Gn − V
 
 
n  ,	n ≥ 1,	(5.7)
 

Cn+1  .	n
 

+ Wn+1,
 


Incremental off-policy every-visit MC policy evaluation
Initialize, for all s ∈ S, a ∈ A(s):
Q(s, a) ← arbitrary
C(s, a) ← 0
µ(a|s) ← an arbitrary soft behavior policy
π(a|s) ← an arbitrary target policy

Repeat forever:
Generate an episode using µ:
S0, A0, R1, . . . , ST −1, AT −1, RT , ST
G ← 0
W ← 1
For t = T − 1, T − 2, . . . downto 0:
G ← γG + Rt+1
C(St, At) ← C(St, At) + W
 
Q(St, At) ← Q(St, At) +	W
 
[G − Q(St, At)]
 
W	W π(At |St )
µ(At |St )
If W = 0 then ExitForLoop
 

 
where C0  .
 

0 (and V1 is arbitrary and thus need not be specified). The box contains
 
a complete episode-by-episode incremental algorithm for Monte Carlo policy evalua- tion. The algorithm is nominally for the off-policy case, using weighted importance sampling, but applies as well to the on-policy case just by choosing the target and behavior policies as the same (in which case (π = µ), W is always 1). The approx- imation Q converges to qπ (for all encountered state–action pairs) while actions are
selected according to a potentially different policy, µ.

Exercise 5.6 Modify the algorithm for first-visit MC policy evaluation (Section 5.1) to use the incremental implementation for sample averages described in Section 2.3.

Exercise 5.7 Derive the weighted-average update rule (5.7) from (5.6). Follow the pattern of the derivation of the unweighted rule (2.3).


5.7	Off-Policy Monte Carlo Control

We are now ready to present an example of the second class of learning control methods we consider in this book: off-policy methods. Recall that the distinguishing feature of on-policy methods is that they estimate the value of a policy while using it for control. In off-policy methods these two functions are separated. The policy used to generate behavior, called the behavior policy, may in fact be unrelated to the policy that is evaluated and improved, called the target policy. An advantage of this separation is that the target policy may be deterministic (e.g., greedy), while the behavior policy can continue to sample all possible actions.
Off-policy Monte Carlo control methods use one of the techniques presented in the preceding two sections. They follow the behavior policy while learning about and improving the target policy. These techniques require that the behavior policy has a nonzero probability of selecting all actions that might be selected by the target policy (coverage). To explore all possibilities, we require that the behavior policy be soft (i.e., that it select all actions in all states with nonzero probability).
The box on the next page shows an off-policy Monte Carlo method, based on GPI and weighted importance sampling, for estimating π∗ and q∗. The target policy
π ≈ π∗  is the greedy policy with respect to Q, which is an estimate of qπ .   The
behavior policy µ can be anything, but in order to assure convergence of π to the
optimal policy, an infinite number of returns must be obtained for each pair of state and action. This can be assured by choosing µ to be ε-soft. The policy π converges to optimal at all encountered states even though actions are selected according to a different soft policy µ, which may change between or even within episodes.
A potential problem is that this method learns only from the tails of episodes, when all of the remaining actions in the episode are greedy. If nongreedy actions are common, then learning will be slow, particularly for states appearing in the early portions of long episodes. Potentially, this could greatly slow learning. There has been insufficient experience with off-policy Monte Carlo methods to assess how seri- ous this problem is. If it is serious, the most important way to address it is probably by incorporating temporal-difference learning, the algorithmic idea developed in the
 
5.7.	OFF-POLICY MONTE CARLO CONTROL	119

Off-policy every-visit MC control (returns π ≈ π∗)
Initialize, for all s ∈ S, a ∈ A(s):
Q(s, a) ← arbitrary
C(s, a) ← 0
π(s) ← a deterministic policy that is greedy with respect to Q

Repeat forever:
Generate an episode using any soft policy µ: S0, A0, R1, . . . , ST −1, AT −1, RT , ST
G ← 0
W ← 1
For t = T − 1, T − 2, . . . downto 0:
G ← γG + Rt+1
C(St, At) ← C(St, At) + W
 
Q(St, At) ← Q(St, At) +	W
 
[G − Q(St, At)]
 
π(St) ← arg maxa Q(St, a)	(with ties broken consistently)
If At /= π(St) then ExitForLoop
W	W	1
µ(At |St )



next chapter. Alternatively, if γ is less than 1, then the idea developed in the next section may also help significantly.

Exercise 5.8: Racetrack (programming) Consider driving a race car around a turn like those shown in Figure 5.6. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments
to the velocity components. Each may be changed by +1, −1, or 0 in one step, for a
total of nine actions. Both velocity components are restricted to be nonnegative and



 
Finish line
 


Finish line
 












 
Starting line
 
Starting line
 

Figure 5.6: A couple of right turns for the racetrack task.
 


less than 5, and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components
zero and ends when the car crosses the finish line. The rewards are −1 for each step
until the car crosses the finish line. If the car hits the track boundary, it is moved
back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car’s location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).




∗5.8	Return-Specific Importance Sampling


The off-policy methods that we have considered so far are based on forming importance- sampling weights for returns considered as unitary wholes, without taking into ac- count the returns’ internal structures as sums of discounted rewards. In this section we briefly consider cutting-edge research ideas for using this structure to significantly reduce the variance of off-policy estimators.
For example, consider the case where episodes are long and γ is significantly less than 1. For concreteness, say that episodes last 100 steps and that γ = 0. The return from time 0 will then be just G0  = R1, but its importance sampling ratio will be
a product of 100 factors,  π(A0|S0) π(A1|S1) • • • π(A99|S99) .  In ordinary importance sam-
 
µ(A0|S0) µ(A1|S1)
 
µ(A99|S99)
 
pling, the return will be scaled by the entire product, but it is really only necessary
to scale by the first factor, by  π(A0|S0) .   The other 99 factors  π(A1|S1) • • • π(A99|S99)
 
µ(A0|S0)
 
µ(A1|S1)
 
µ(A99|S99)
 
are irrelevant because after the first reward the return has already been determined.
These later factors are all independent of the return and of expected value 1; they do not change the expected update, but they add enormously to its variance. In some cases they could even make the variance infinite. Let us now consider an idea for avoiding this large extraneous variance.

The essence of the idea is to think of discounting as determining a probability of termination or, equivalently, a degree of partial termination. For any γ ∈ [0, 1), we can think of the return G0  as partly terminating in one step, to the degree 1 − γ,
producing a return of just the first reward, R1, and as partly terminating after two steps, to the degree (1 − γ)γ, producing a return of R1 + R2, and so on. The latter degree corresponds to terminating on the second step, 1 − γ, and not having already
terminated on the first step, γ. The degree of termination on the third step is thus (1 − γ)γ2, with the γ2 reflecting that termination did not occur on either of the first
 
∗5.8.  RETURN-SPECIFIC IMPORTANCE SAMPLING	121
two steps. The partial returns here are called flat partial returns:
¯h = Rt+1 + Rt+2 + • • • + Rh,	0 ≤ t < h ≤ T,
where “flat” denotes the absence of discounting, and “partial” denotes that these returns do not extend all the way to termination but instead stop at h, called the horizon (and T is the time of termination of the episode). The conventional full return Gt can be viewed as a sum of flat partial returns as suggested above as follows:
= Rt+1 + γRt+2 + γ2Rt+3 + • • • + γT −t−1RT
= (1 − γ)Rt+1
+ (1 − γ)γ (Rt+1 + Rt+2)
+ (1 − γ)γ2 (Rt+1 + Rt+2 + Rt+3)

.
 
+ (1 − γ)γT −t−2 (Rt+1 + Rt+2 + • • • + RT
+ γT −t−1 (Rt+1 + Rt+2 + • • • + RT )
T −1
 

−1)
 
= (1 − γ) ... γh−t−1G¯h
 
+  γT −t−1G¯T
 
h=t+1

Now we need to scale the flat partial returns by an importance sampling ratio that is similarly truncated. As Gh only involves rewards up to a horizon h, we only need
the ratio of the probabilities up to h.  We define an ordinary importance-sampling estimator, analogous to (5.4), as
 
>-	(1 − γ) >-T (t)−1 γh−t−1ρhG¯h
 
+  γT (t)−t−1ρT (t)G¯T (t) 
 
V (s) .
 
    t∈T(s) 	h=t+1 	t    t 	t 	t 	
|T(s)|
 
, (5.8)
 
and a weighted importance-sampling estimator, analogous to (5.5), as
 
>-	(1 − γ) >-T (t)−1 γh−t−1ρhG¯h
 
+  γT (t)−t−1ρT (t)G¯T (t) 
 
V (s) .
 
    t∈T(s) 	h=t+1 	t    t 	t 	t 	
 
=
>-
t∈T(s)
 
(1 − γ) >-T (t)−1
 
h−t−1
 
ρh	+   γ
 
T (t)−t−1
 
T (t)	. (5.9)
ρt
 

We call these two estimators discounting-aware importance sampling estimators. They take into account the discount rate but have no affect (are the same as the off-policy estimators from Section 5.5) if γ = 1.
There is one more way in which the structure of the return as a sum of rewards can be taken into account in off-policy importance sampling, a way that may be able to reduce variance even in the absence of discounting (that is, even if γ = 1). In the off-policy estimators (5.4) and (5.5), each term of the sum in the numerator is itself a sum:
 
t Gt = ρt  rRt+1 + γRt+2 + • • • + γ
 
RT )
 
ρT	T
 
T −t−1
 
= ρT Rt+1 + γρT Rt+2 + • • • + γT −t−1ρT RT .	(5.10)
t	t	t
 


The off-policy estimators rely on the expected values of these terms; let us see if we can write them in a simpler way. Note that each sub-term of (5.10) is a product of a random reward and a random importance-sampling ratio. For example, the first sub-term can be written, using (5.3), as

 
ρT	π(At|St) π(At+1|St+1) π(At+2|St+2)
 
π(AT −1|ST −1)
 
t Rt+1 = µ(A |S ) µ(A	|S
 
) µ(A	|S
 
) • • • µ(A	|S
 
Rt+1.
)
 
t	t	t+1
 
t+1
 
t+2
 
t+2
 
T −1
 
T −1
 

Now notice that,  of all these factors,  only the first and the last (the reward) are correlated; all the other ratios are independent random variables whose expected value is one:
 
i π(Ak |Sk ) = ... µ(a S
 
)	= ... π(a S
 

) = 1.
 
EAk ∼µ
 
µ(Ak |Sk )
 
|  k  µ(a|Sk )	| k
 

Thus, because the expectation of the product of independent random variables is the product of their expectations, all the ratios except the first drop out in expectation, leaving just
ErρT Rt+1l = Erρt+1Rt+1l .
t	t

If we repeat this analysis for the kth term of (5.10), we get

ErρT Rt+k l = E ρt+k Rt+k  .
t	t

It follows then that the expectation of our original term (5.10) can be written

ErρT Gtl = E G˜t  ,

 
where

G˜t = ρt+1
 


t+2
 


2  t+3
 


T −t−1   T
 
t	Rt+1 + γρt	Rt+2 + γ
 
ρt	Rt+3 + • • • + γ
 
ρt RT .
 

We call this idea per-reward importance sampling.  It follows immediately that there is an alternate importance-sampling estimator, with the same unbiased expectation
as the OIS estimator (5.4), using G˜t:
>-	G˜t
 
V (s) .
 
    t  T(s) 	
,	(5.11)
|T(s)|
 

which we might expect to sometimes be of lower variance.
Is there a per-reward version of weighted importance sampling? This is less clear. So far, all the estimators that have been proposed for this that we know of are not consistent (that is, they do not converge to the true value with infinite data).
∗Exercise 5.9 Modify the algorithm for off-policy Monte Carlo control (page 119) to use the idea of the truncated weighted-average estimator (5.9). Note that you will first need to convert this equation to action values.
 

5.9    Summary


The Monte Carlo methods presented in this chapter learn value functions and op- timal policies from experience in the form of sample episodes. This gives them at least three kinds of advantages over DP methods. First, they can be used to learn optimal behavior directly from interaction with the environment, with no model of the environment’s dynamics. Second, they can be used with simulation or sample models. For surprisingly many applications it is easy to simulate sample episodes even though it is difficult to construct the kind of explicit model of transition proba- bilities required by DP methods. Third, it is easy and efficient to focus Monte Carlo methods on a small subset of the states. A region of special interest can be accurately evaluated without going to the expense of accurately evaluating the rest of the state set (we explore this further in Chapter 8).
A fourth advantage of Monte Carlo methods, which we discuss later in the book, is that they may be less harmed by violations of the Markov property. This is because they do not update their value estimates on the basis of the value estimates of successor states. In other words, it is because they do not bootstrap.
In designing Monte Carlo control methods we have followed the overall schema of generalized policy iteration (GPI) introduced in Chapter 4. GPI involves interacting processes of policy evaluation and policy improvement. Monte Carlo methods provide an alternative policy evaluation process. Rather than use a model to compute the value of each state, they simply average many returns that start in the state. Because a state’s value is the expected return, this average can become a good approximation to the value. In control methods we are particularly interested in approximating action-value functions, because these can be used to improve the policy without requiring a model of the environment’s transition dynamics. Monte Carlo methods intermix policy evaluation and policy improvement steps on an episode-by-episode basis, and can be incrementally implemented on an episode-by-episode basis.
Maintaining sufficient exploration  is an issue in Monte Carlo control methods. It is not enough just to select the actions currently estimated to be best, because then no returns will be obtained for alternative actions, and it may never be learned that they are actually better. One approach is to ignore this problem by assuming that episodes begin with state–action pairs randomly selected to cover all possibilities. Such exploring starts can  sometimes be arranged in  applications with  simulated episodes, but are unlikely in learning from real experience. In on-policy  methods, the agent commits to always exploring and tries to find the best policy that still explores. In off-policy methods, the agent also explores, but learns a deterministic optimal policy that may be unrelated to the policy followed.
Off-policy prediction refers to learning the value function of a target policy from data generated by a different behavior policy. Such learning methods are based on some form of importance sampling, that is, on weighting returns by the ratio of the probabilities of taking the observed actions under the two policies. Ordinary im- portance sampling uses a simple average of the weighted returns, whereas weighted importance sampling uses a weighted average.  Ordinary importance sampling pro-
 


duces unbiased estimates, but has larger, possibly infinite, variance, whereas weighted importance sampling always has finite variance and is preferred in practice. Despite their conceptual simplicity, off-policy Monte Carlo methods for both prediction and control remain unsettled and are a subject of ongoing research.
The Monte Carlo methods treated in this chapter differ from the DP methods treated in the previous chapter in two major ways. First, they operate on sample experience, and thus can be used for direct learning without a model. Second, they do not bootstrap. That is, they do not update their value estimates on the basis of other value estimates. These two differences are not tightly linked, and can be separated. In the next chapter we consider methods that learn from experience, like Monte Carlo methods, but also bootstrap, like DP methods.


Bibliographical and Historical Remarks

The term “Monte Carlo” dates from the 1940s, when physicists at Los Alamos de- vised games of chance that they could study to help understand complex physical phenomena relating to the atom bomb. Coverage of Monte Carlo methods in this sense can be found in several textbooks (e.g., Kalos and Whitlock, 1986; Rubinstein, 1981).
An early use of Monte Carlo methods to estimate action values in a reinforcement learning context was by Michie and Chambers (1968). In pole balancing (Example 3.4), they used averages of episode durations to assess the worth (expected balancing “life”) of each possible action in each state, and then used these assessments to control action selections. Their method is similar in spirit to Monte Carlo ES with every- visit MC estimates. Narendra and Wheeler (1986) studied a Monte Carlo method for ergodic finite Markov chains that used the return accumulated between successive visits to the same state as a reward for adjusting a learning automaton’s action probabilities.
Barto and Duff (1994) discussed policy evaluation in the context of classical Monte Carlo algorithms for solving systems of linear equations. They used the analysis of Curtiss (1954) to point out the computational advantages of Monte Carlo policy eval- uation for large problems. Singh and Sutton (1996) distinguished between every-visit and first-visit MC methods and proved results relating these methods to reinforce- ment learning algorithms.
The blackjack example is based on an example used by Widrow, Gupta, and Maitra (1973). The soap bubble example is a classical Dirichlet problem whose Monte Carlo solution was first proposed by Kakutani (1945; see Hersh and Griego, 1969; Doyle and Snell, 1984). The racetrack exercise is adapted from Barto, Bradtke, and Singh (1995), and from Gardner (1973).
Monte Carlo ES was introduced in the 1998 edition of this book. That may have been the first explicit connection between Monte Carlo estimation and control methods based on policy iteration.
Efficient off-policy learning has become recognized as an important challenge that
 


arises in several fields. For example, it is closely related to the idea of “interventions” and “counterfactuals” in probabalistic graphical (Bayesian) models (e.g., Pearl, 1995; Balke and Pearl, 1994). Off-policy methods using importance sampling have a long history and yet still are not well understood. Weighted importance sampling, which is also sometimes called normalized importance sampling (e.g., Koller and Friedman, 2009), is discussed by Rubinstein (1981), Hesterberg (1988), Shelton (2001), and Liu
(2001) among others.
Our treatment of the idea of discounting-aware importance sampling is based on the analysis of Sutton, Mahmood, Precup, and van Hasselt (2014). It has been worked out most fully to date by Mahmood (in preparation; Mahmood, van Hasselt, and Sutton, 2014). Per-reward importance sampling was introduced by Precup, Sutton, and Singh (2000), who called it “per-decision” importance sampling. These works also combine off-policy learning with temporal-difference learning,  eligibility traces, and approximation methods, introducing subtle issues that we consider in later chapters.
The target policy in off-policy learning is sometimes referred to in the literature as the “estimation” policy, as it was in the first edition of this book.
