Chapter 2
Multi-arm Bandits
The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active ex- ploration, for an explicit trial-and-error search for good behavior. Purely evaluative feedback indicates how good the action taken is, but not whether it is the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning, which includes large parts of pat- tern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the ac- tion taken. There are also interesting intermediate cases in which evaluation and instruction blend together.
In this chapter we study the evaluative aspect of reinforcement learning in a sim- plified setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforce- ment learning problem. Studying this case will enable us to see most clearly how evaluative feedback differs from, and yet can be combined with, instructive feedback.
The particular nonassociative, evaluative feedback problem that we explore is a simple version of the k-armed bandit problem. We use this problem to introduce a number of basic learning methods which we extend in later chapters to apply to the full reinforcement learning problem. At the end of this chapter, we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when actions are taken in more than one situation.
27
28 CHAPTER 2. MULTI-AR BANDITS
2.1 A k-Armed Bandit Problem
Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.
This is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or “one-armed bandit,” except that it has k levers instead of one. Each action selection is like a play of one of the slot machine’s levers, and the rewards are the payoffs for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers. Another analogy is that of a doctor choosing between experimental treatments for a series of seriously ill patients. Each action selection is a treatment selection, and each reward is the survival or well-being of the patient. Today the term “bandit problem” is sometimes used for a generalization of the problemdescribed above, but in this book we use it to refer just to this simple case.
In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q∗(a), is the expected reward given that a is selected:
If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We as- sume that you do not know the action values with certainty, although you may have
estimates. We denote the estimated value of action a at time t as Qt(a) ≈ q∗(a).
If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action’s value. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. For example, suppose a greedy action’s value is known with certainty, while several other actions are estimated to be nearly as good but with substantial uncertainty. The uncertainty is such that at least one of these other actions probably is actually better than the greedy action, but you don’t know which one. If you have many time steps ahead on which to make action selections, then it may be better to explore the nongreedy actions and discover which of them are better than the greedy action. Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore
2.2. ACTION-VALUE METHODS 29
and to exploit with any single action selection, one often refers to the “conflict” between exploration and exploitation.
In any specific case, whether it is better to explore or exploit depends in a com- plex way on the precise values of the estimates, uncertainties, and the number of remaining steps. There are many sophisticated methods for balancing exploration and exploitation for particular mathematical formulations of the k-armed bandit and related problems. However,most of these methods make strong assumptions about stationarity and prior knowledge that are either violated or impossible to verify in applications and in the full reinforcement learning problem that we consider in sub- sequent chapters. The guarantees of optimality or bounded loss for these methods are of little comfort when the assumptions of their theory do not apply.
In this book we do not worry about balancing exploration and exploitation in a sophisticated way; we worry only about balancing them at all. In this chapter we present several simple balancing methods for the k-armed bandit problem and show that they work much better than methods that always exploit. The need to balance exploration and exploitation is a distinctive challenge that arises in reinforcement learning; the simplicity of the k-armed bandit problem enables us to show this in a particularly clear form.
2.2 Action-Value Methods
We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received:
where 1predicate denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define Qt(a) as some default value, such as Q1(a) = 0. As the denominator goes to infinity, by the law of large numbers, Qt(a) converges to q∗(a). We call this the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards. Of course this is just one way to estimate action values, and not necessarily the best one. Nevertheless, for now let us stay with this simple estimation method and turn to the question of how the estimates might be used to select actions.
The simplest action selection rule is to select the action (or one of the actions) with highest estimated action value, that is, to select at step t one of the greedy
actions,A∗,forwhichQt(A∗)=maxa Qt(a). Thisgreedy actionselectionmethod
t t
can be written as
30 CHAPTER 2. MULTI-ARM BANDITS
such as that shown in Figure 2.1, the action values, q∗(a), a = 1, . . . , 10, were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. Then, when a learning method applied to that problem selected action At at time t, the
where argmaxa denotes the value of a at which the expression that follows is maxi- mized (with ties broken arbitrarily). Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling appar- ently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probabil- ity ε, instead to select randomly from amongst all the actions with equal probability independently of the action-value estimates. We callmethods using this near-greedy action selection rule ε-greedy methods. An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the Qt(a) converge to q∗(a). This of course implies that the probability of selecting the optimal action converges to greater than
1 − ε, that is, to near certainty. These are just asymptotic guarantees, however, and
say little about the practical effectiveness of the methods.
To roughly assess the relative effectiveness of the greedy and ε-greedy methods, we compared them numerically on a suite of test problems. This was a set of 2000 randomly generated k-armed bandit problems with k = 10. For each bandit problem,
Figure 2.1: An exemplary bandit problem from the 10-armed testbed. The true value q∗(a) of each of the ten actions was selected according to a normal distribution around zero with unit variance, and then the actual rewards were selected around q∗(a) with unit variance, as suggested by these gray distributions.
2.2. ACTION-VALUE METHODS 31
actual reward Rt was selected from a normal distribution with mean q∗(At) and variance 1. It is these distributions which are shown as gray in Figure 2.1. We call this suite of test tasks the 10-armed testbed. For any learning method, we can measure its performance and behavior as it improves with experience over 1000 steps interacting with one of the bandit problem. This makes up one run. Repeating this for 2000 independent runs with a different bandit problem, we obtained measures of the learning algorithm’s average behavior.
Figure 2.2 compares a greedy method with two ε-greedy methods (ε = 0.01 and ε = 0.1), as described above, on the 10-armed testbed. Both methods formed their action-value estimates using the sample-average technique. The upper graph shows the increase in expected reward with experience. The greedy method improved slightly faster than the other methods at the very beginning, but then leveled off at a lower level. It achieved a reward per step of only about 1, compared with the best possible of about 1.55 on this testbed. The greedy method performs significantly worse in the long run because it often gets stuck performing suboptimal actions. The lower graph shows that the greedy method found the optimal action in only approximately one-third of the tasks. In the other two-thirds, its initial samples of the optimal action were disappointing, and it never returned to it. The ε-greedy methods eventually perform better because they continue to explore and to improve
Figure 2.2: Average performance of ε-greedy action-value methods on the 10-armed testbed. These data are averages over 2000 runs with different bandit problem. All methods used sample averages as their action-value estimates.
32 CHAPTER 2. MULTI-ARM BANDITS
their chances of recognizing the optimal action. The ε = 0.1 method explores more, and usually finds the optimal action earlier, but never selects it more than 91% of the time. The ε = 0.01 method improves more slowly, but eventually would perform better than the ε = 0.1 method on both performance measures. It is also possible to reduce ε over time to try to get the best of both high and low values.
Theadvantageof ε-greedy overgreedymethodsdependsonthetask. Forexample, suppose the reward variance had been larger, say 10 instead of 1. With noisier rewards it takes more exploration to find the optimal action, and ε-greedy methods should fare even better relative to the greedy method. On the other hand, if the reward variances were zero, then the greedy method would know the true value of each action after trying it once. In this case the greedy method might actually perform best because it would soon find the optimal action and then never explore. But even in the deterministic case, there is a large advantage to exploring if we weaken some of the other assumptions. For example, suppose the bandit task were nonstationary, that is, that the true values of the actions changed over time. In this case exploration is needed even in the deterministic case to make sure one of the nongreedy actions has not changed to become better than the greedy one. As we will see in the next few chapters, effective nonstationarity is the case most commonly encountered in reinforcement learning. Even if the underlying task is stationary and deterministic, thelearnerfaces a set ofbanditlike decisiontaskseach ofwhich changes over time as learning proceeds and the agent’s policy changes. Reinforcement learning requires a balance between exploration and exploitation.
Exercise 2.1 In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and cumulative probability of selecting the best action? How much better will it be? Express your answer quantitatively.
2.3 Incremental Implementation
The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. We now turn to the question of how these averages can be computed in a computationally efficient manner, in particular, with constant memory and per-time-step computation.
To simplify notation we concentrate on a single action. Let Ri now denote the reward received after the ith selection of this action, and let Qn denote the estimate
of its action value after it has been selected n − 1 times, which we can now write
simply as
The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, in this case the memory and computational requirements would grow over time as more rewards are seen. Each additional reward would require more memory to store it and more computation to compute the sum in the numerator.
2.3. INCREMENTAL IMPLEMENTATION 33
As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. Given Qn and the nth reward, Rn, the new average of all n rewards can be computed by
which holds even for n = 1, obtaining Q2 = R1 for arbitrary Q1. This implemen- tation requires memory only for Qn and n, and only the small computation (2.3) for each new reward. Pseudocode for a complete bandit algorithm using incremen- tally computed sample averages and ε-greedy action selection is shown below. The function bandit(a) is assumed to take an action and return a corresponding reward.
The update rule (2.3) is of a form that occurs frequently throughout this book. The general form is
The expression Target − OldEstimateis an error in the estimate. It is reduced by taking a step toward the “Target.” The target is presumed to indicate a desirable direction inwhich to move, though it may be noisy. In the case above, for example, the target is the nth reward.
34 CHAPTER 2. MULTI-ARM BANDITS
n
n
Note that the step-size parameter (StepSize) used in the incremental method described above changes from time step to time step. In processing the nth reward for action a, that method uses a step-size parameter of 1 . In this book we denote the step-size parameter by the symbol α or, more generally, by αt(a). We sometimes
use the informal shorthand α = 1
to refer to this case, leaving the dependence of n
on the action implicit, just as wehave in this section.
2.4 Tracking a Nonstationary Problem
The averagingmethods discussed so far are appropriate in a stationary environment, but not if the bandit is changing over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to weight recent rewards more heavily than long-past ones. One of the most popular ways of doing this is to use a constant step-size parameter. For
example, the incremental update rule (2.3) for updating an average Qn of the n − 1
past rewards is modified to be
where the step-size parameter α ∈ (0, 1]1 is constant. This results in Qn+1 being a
weighted average of past rewards and the initial estimate Q1:
We call this a weighted average because the sum of the weights is (1−α)n+
α)n−i = 1, as you can check for yourself. Note that the weight, α(1 − α)n−i, given
to the reward Ri depends on how many rewards ago, n − i, it was observed. The
quantity 1 − α is less than 1, and thus the weight given to Ri decreases as the number
of intervening rewards increases. In fact, the weight decays exponentially according to the exponent on 1 − α. (If 1 − α = 0, then all the weight goes on the very last
reward, Rn, because of the convention that 00 = 1.) Accordingly, this is sometimes called an exponential, recency-weighted average.
Sometimes it is convenient to vary the step-size parameter from step to step. Let
αn(a) denote the step-size parameter used to process the reward received after the
1The notation (a, b] as a set denotes the real interval between a and b including b but not including
a.Thus, here we are saying that 0 < α ≤ 1.
n
1
n
2.5. OPTIMISTIC INITIAL VALUES 35
nth selection of action a. As we have noted, the choice αn(a) = 1
results in the
sample-average method, which is guaranteed to converge to the true action values by
the law of large numbers. But of course convergence is not guaranteed for all choices of the sequence {αn(a)}. A well-known result in stochastic approximation theory
gives us the conditions required to assure convergence with probability 1:
The first condition is required to guarantee that the steps are large enough to even- tually overcome any initial conditions or random fluctuations. The second condition guaranteesthat eventuallythesteps becomesmallenoughtoassure convergence.
Notethat both convergenceconditionsaremetforthesample-average case, αn(a) =
n , but not for the case of constant step-size parameter, αn(a) = α. In the latter case, the second condition is not met, indicating that the estimates never completely con-
verge but continue to vary in response to the most recently received rewards. As we mentioned above, this is actually desirable in a nonstationary environment, and problems that are effectively nonstationary are the norm in reinforcement learn- ing. In addition, sequences of step-size parameters that meet the conditions (2.7) often converge very slowly or need considerable tuning in order to obtain a satisfac- tory convergence rate. Although sequences of step-size parameters that meet these convergence conditions are often used in theoretical work, they are seldom used in applications and empirical research.
Exercise 2.2 If the step-size parameters, αn, are not constant, then the estimate Qn is a weighted average of previously received rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters?
Exercise 2.3 (programming) Design and conduct an experiment to demonstrate the difficulties that sample-averagemethods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the q∗(a) start out equal and then take independent random walks. Prepare plots like Figure 2.2 for an action- valuemethod using sample averages, incrementally computed by α = 1 , and another action-value method using a constant step-size parameter, α = 0.1. Use ε = 0.1 and, if necessary, runs longer than 1000 steps.
2.5 Optimistic Initial Values
All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, Q1(a). In the language of statistics, these methods are biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant α, the bias is permanent, though decreasing over time as given by (2.6). In practice, this kind of bias is usually not a problem and can sometimes be very helpful. The downside is that the initial estimates become, in effect, a set of parameters that must be picked
36 CHAPTER 2. MULTI-ARM BANDITS
Plays
by the user, if only to set them all to zero. The upside is that they provide an easy way to supply some prior knowledge about what level of rewards can be expected.
Initial action values can also be used as a simple way of encouraging exploration. Suppose that instead of setting the initial action values to zero, as we did in the 10-armed testbed, we set them all to +5. Recall that the q∗(a) in this problem are selected from a normal distribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic. But this optimism encourages action-value methods to explore. Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being “disappointed” with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time.
Figure 2.3 shows the performance on the 10-armed bandit testbed of a greedy method using Q1(a) = +5, for all a. For comparison, also shown is an ε-greedy method with Q1(a) = 0. Initially, the optimistic method performs worse because it explores more, but eventually it performs better because its exploration decreases with time. We call this technique for encouraging exploration optimistic initial val- ues. We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for ex- ploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial state in any special way is unlikely to help with the general nonstationary case. The beginning of time occurs only once, and thus we should not focus on it too much. This criticism applies as well to the sample-average methods, which also treat the beginning of time as a special event, averaging all subsequent rewards with equal weights. Nevertheless, all of thesemethods are very simple, and one of themor some simple combination of them is often adequate in practice. In the rest of this book we make frequent use of several of these simple exploration techniques.
Steps
Figure2.3: Theeffect ofoptimistic initialaction-valueestimatesonthe10-armedtestbed. Both methods used a constant step-size parameter, α = 0.1.
2.6. UPPER-CONFIDENCE-BOUND ACTION SELECTION 37
Exercise 2.4 The results shown in Figure 2.3 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?
2.6 Upper-Confidence-Bound Action Selection
Exploration is needed because the estimates of the action values are uncertain. The greedy actions are those that look best at present, but some of the other actions may actually be better. ε-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions as
where log t denotes the natural logarithm of t (the number that e ≈ 2.71828 would
have to be raised to in order to equal t), Nt(a) denotes the number of times that
action a has been selected prior to time t (the denominator in (2.1)), and the number
c > 0 controls the degree of exploration. If Nt(a) = 0, then a is considered to be a maximizing action.
The idea of this upper confidence bound (UCB) action selection is that the square- root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with the c parameter determining the confidence level. Each time a is selected the uncertainty is presumably reduced; Nt(a) is incremented and, as it appears in the denominator of the uncertainty term, the term is decreased. On the other hand, each time an action other than a is selected t is increased but Nt(a) is not; as t appears in the numerator the uncertainty estimate is increased. The use of the natural logarithm means that the increase gets smaller over time, but is unbounded; all actions will eventually be selected, but as time goes by it will be a longer wait, and thus a lower selection frequency, for actions with a lower value estimate or that have already been selectedmore times.
Results with UCB on the 10-armed testbed are shown in Figure 2.4. UCB will often perform well, as shown here, but is more difficult than ε-greedy to extend beyond bandits to themore general reinforcement learning settings considered in the rest of this book. One difficulty is in dealing with nonstationary problems; something more complex than the methods presented in Section 2.4 would be needed. Another difficulty is dealing with large state spaces, particularly function approximation as developed in Part II of this book. In these more advanced settings there is currently
38 CHAPTER 2. MULTI-ARM BANDITS
Figure 2.4: Average performance of UCB action selection on the 10-armed testbed. As shown, UCB generally performs better than ε-greedy action selection, except in the first k steps, when it selects randomly among the as-yet-untried actions.
no known practical way of utilizing the idea of UCB action selection.
Exercise 2.5 In Figure 2.4 the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward increases on the 11th step and why it decreases on the subsequent steps. Hint: if c = 1, then the spike is much less prominent.
2.7 Gradient Bandit Algorithms
So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. In this section we consider learning a numerical preference Ht(a) for each action a. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:
where here we have also introduced a useful new notation πt(a) for the probability of taking action a at time t. Initially all preferences are the same (e.g., H1(a) = 0, ∀a)
so that all actions have an equal probability of being selected.
There is a natural learning algorithm for this setting based on the idea of stochastic gradient ascent. On each step, after selecting the action At and receiving the reward
2.7. GRADIENT BANDIT ALGORITHMS 39
where α > 0 is a step-size parameter, and R¯t ∈ R is the average of all the rewardsup through and including time t, which can be computed incrementally as described in Section 2.3 (or Section 2.4 if the problem is nonstationary). The R¯t term servesas a baseline with which the reward is compared. If the reward is higher than the
baseline, then the probability of taking At in the future is increased, and if the reward is below baseline, then probability is decreased. The non-selected actions move in the opposite direction.
Figure 2.5 shows results with the gradient bandit algorithm on a variant of the 10-armed testbed in which the true expected rewards were selected according to a normal distribution with a mean of +4 instead of zero (and with unit variance as before). This shifting up of all the rewards has absolutely no effect on the gradient bandit algorithm because of the reward baseline term, which instantaneously adapts
to the new level. But if the baseline were omitted (that is, if R¯t was taken to be
constant zero in (2.10)), then performance would be significantly degraded, as shown
in the figure.
Figure 2.5: Average performance of the gradient bandit algorithm with and without a reward baseline on the 10-armed testbed when the q∗(a) are chosen to be near +4 rather than near zero.
The Bandit Gradient Algorithm as Stochastic Gradient Ascent
One can gain a deeper insight into the gradient bandit algorithm by under- standing it as a stochastic approximation to gradient ascent. In exact gradient ascent, each preference Ht(a) would be incrementing proportional to the in- crement’s effect on performance:
40 CHAPTER 2. MULTI-ARM BANDITS
and the measure of the increment’s effect is the partial derivative of this per- formancemeasure with respect to the preference. Of course, it is not possible to implement gradient ascent exactly in our case because by assumption we do not know the q∗(b), but in fact the updates of our algorithm (2.10) are equal to (2.11) in expected value, making the algorithm an instance of stochastic gra- dient ascent. The calculations showing this require only beginning calculus, but take several steps. First we take a closer look at the exact performance gradient:
Ht(a) is changed, some actions’ probabilities go up and some down, but the
sum of the changes must be zero because the sum of the probabilities must
The equation is now in the form of an expectation, summing over all possible values b of the random variable At, then multiplying by the probability of taking those values. Thus:
where here we have chosen Xt =
R¯t and substituted Rt for q∗(At), which
is permitted because E[Rt|At] = q∗(At) and because the Rt (given At) is
uncorrelated with anything else. Shortly we will establish that ∂ πt(b) =
Recall that our plan has been to write the performance gradient as an expecta- tion of something that we can sample on each step, as we have just done, and
2.7. GRADIENT BANDIT ALGORITHMS 41
∂H (a)
then update on each step proportional to the sample. Substituting a sample of the expectation above for the performance gradient in (2.11) yields:
which you will recognize as being equivalent to our original algorithm (2.10).
Thus it remains only to show that ∂ πt(b)
t
= πt(b)r1a=b − πt(a)), as we
assumed. Recall the standard quotient rule for derivatives:
We have just shown that the expected update of the gradient bandit algo- rithm is equal to the gradient of expected reward, and thus that the algorithm is an instance of stochastic gradient ascent. This assures us that the algorithm has robust convergence properties.
Note that we did not require any properties of the reward baseline other than that it does not depend on the selected action. For example, we could have set it to zero, or to 1000, and the algorithm would still be an instance of stochastic gradient ascent. The choice of the baseline does not affect the expected update of the algorithm, but it does affect the variance of the update and thus the rate of convergence (as shown, e.g., in Figure 2.5). Choosing it as the average of the rewards may not be the very best, but it is simple and works well in practice.
42 CHAPTER 2. MULTI-ARM BANDITS
2.8 Associative Search (Contextual Bandits)
So far in this chapter we have considered only nonassociative tasks, in which there is no need to associate different actions with different situations. In these tasks the learner either tries to find a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary. However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations. To set the stage for the full problem, we briefly discuss the simplest way in which nonassociative tasks extend to the associative setting.
As an example, suppose there are several different k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. This would appear to you as a single, nonstationary k-armed bandit task whose true action values change randomly from step to step. You could try using one of the methods described in this chapter that can handle nonstationarity, but unless the true action values change slowly, these methods will not work very well. Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that task—for instance, if red, select arm 1; if green, select arm 2. With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another.
This is an example of an associative search task, so called because it involves both trial-and-error learning in the form of search for the best actions and association of these actions with the situations in which they are best.2 Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem. We present this problem in the next chapter and consider its ramifications throughout the rest of the book.
2.9 Summary
We have presented in this chapter several simple ways of balancing exploration and exploitation. The ε-greedy methods choose randomly a small fraction of the time, whereas UCB methods choose deterministically but achieve exploration by subtly favoring at each step the actions that have so far received fewer samples. Gradient bandit algorithms estimate not action values, but action preferences, and favor the
2Associative search tasks are often now termed contextual bandits in the literature.
2.9. SUMMARY 43
more preferred actions in a graded, probabilistic manner using a soft-max distribu- tion. The simple expedient of initializing estimates optimistically causes even greedy methods to explore significantly.
It is natural to ask which of these methods is best. Although this is a difficult question to answer in general, we can certainly run them all on the 10-armed testbed that we have used throughout this chapter and compare their performances. A complication is that they all have a parameter; to get a meaningful comparison we will have to consider their performance as a function of their parameter. Our graphs so far have shown the course of learning over time for each algorithm and parameter setting, but it would be too visually confusing to show such a learning curve for each algorithm and parameter value. Instead we summarize a complete learning curve by its average value over the 1000 steps; this value is proportional to the area under the learning curves we have shown up to now. Figure 2.6 shows this measure for the various bandit algorithms from this chapter, each as a function of its own parameter shown on a single scale on the x-axis. Note that the parameter values are varied by factors of two and presented on a log scale. Note also the characteristic inverted- U shapes of each algorithm’s performance; all the algorithms perform best at an intermediate value of their parameter, neither too large nor too small. In assessing a method, we should attend not just to how well it does at its best parameter setting, but also to how sensitive it is to its parameter value. All of these algorithms are fairly insensitive, performing well over a range of parameter values varying by about an order of magnitude. Overall, on this problem, UCB seems to perform best.
Despite their simplicity, in our opinion the methods presented in this chapter can fairly be considered the state of the art. There are more sophisticated methods, but their complexity and assumptions make them impractical for the full reinforcement learning problem that is our real focus. Starting in Chapter 5 we present learning methods for solving the full reinforcement learning problem that use in part the
Figure 2.6: A parameter study of the various bandit algorithms presented in this chapter. Each point is the average reward obtained over 1000 steps with a particular algorithm at a particular setting of its parameter.
44 CHAPTER 2. MULTI-ARM BANDITS
simple methods explored in this chapter.
Although the simple methods explored in this chapter may be the best we can do at present, they are far from a fully satisfactory solution to the problem of balancing exploration and exploitation.
The classical solution to balancing exploration and exploitation in k-armed bandit problems is to compute special functions called Gittins indices. These provide an optimal solution to a certain kind of bandit problem more general than that con- sidered here but that assumes the prior distribution of possible problems is known. Unfortunately, neither the theory nor the computational tractability of this method appear to generalize to the full reinforcement learning problem that we consider in the rest of the book.
Bayesian methods assume a known initial distribution over the action values and then updates the distribution exactly after each step (assuming that the true action values are stationary). In general, the update computations can be very complex, but for certain special distributions (called conjugate priors) they are easy. One possibility is to then select actions at each step according to their posterior proba- bility of being the best action. This method, sometimes called posterior sampling or Thompson sampling, often performs similarly to the best of the distribution-free methods we have presented in this chapter.
In the Bayesiansettingit is evenconceivabletocomputetheoptimal balance be- tween exploration and exploitation. Clearly, for any possible action we can compute theprobability ofeach possibleimmediaterewardandtheresultant posteriordistri- butions over action values. This evolving distribution becomes the information state of the problem. Given a horizon, say of 1000 steps, one can consider all possible actions, all possible resulting rewards, all possible next actions, all next rewards, and so on for all 1000 steps. Given the assumptions, the rewards and probabilities of each possible chain of events can be determined,andoneneedonly pick the best. But the tree of possibilities grows extremely rapidly; even if there are only two ac- tions and two rewards, the tree will have 22000 leaves. It is generally not feasible to perform this immense computation exactly, but perhaps it could be approximated efficiently. Thisapproach wouldeffectively turnthebanditproblem intoaninstance of the full reinforcement learning problem; it is beyond the current state of the art, but someday it may be possible to use reinforcement learningmethods such as those presented in Part II of this book to approximate this optimal solution.
Bibliographical and Historical Remarks
2.1 Bandit problems have been studied in statistics, engineering, and psychology.
In statistics, bandit problems fall under the heading “sequential design of ex- periments,” introduced by Thompson (1933, 1934) and Robbins (1952), and studied by Bellman (1956). Berry and Fristedt (1985) provide an extensive treatment of bandit problems from the perspective of statistics. Narendra and Thathachar (1989) treat bandit problems from the engineering perspec-
2.9. SUMMARY 45
tive, providing a good discussion of the various theoretical traditions that have focused on them. In psychology, bandit problems have played roles in statistical learning theory (e.g., Bush and Mosteller, 1955; Estes, 1950).
The term greedy is often used in the heuristic search literature (e.g., Pearl, 1984). The conflict between exploration and exploitation is known in control engineering as the conflict between identification (or estimation) and control (e.g., Witten, 1976). Feldbaum (1965) called it the dual control problem, referring to the need to solve the two problems of identification and con- trol simultaneously when trying to control a system under uncertainty. In discussing aspects of genetic algorithms, Holland (1975) emphasized the im- portance of this conflict, referring to it as the conflict between the need to exploit and the need for new information.
2.2 Action-value methods for our k-armed bandit problem were first proposed by Thathachar and Sastry (1985). These are often called estimator algorithms in the learning automata literature. The term action value is due to Watkins (1989). Thefirst to use ε-greedymethods may also have been Watkins (1989,
p.187), but the idea is so simple that some earlier use seems likely.
2.3–4 This material falls under the general heading of stochastic iterative algo- rithms, which is well covered by Bertsekas and Tsitsiklis (1996).
2.5 Optimistic initialization was used in reinforcement learning by Sutton (1996).
2.6 Early work on using estimates of the upper confidence bound to select actions was done by Lai and Robbins (1985), Kaelbling (1993b), and Agrawal (1995). The UCB algorithm we present here is called UCB1 in the literature and was first developed by Auer, Cesa-Bianchi and Fischer (2002).
2.7 Gradient bandit algorithms are a special case of the gradient-based rein- forcement learning algorithms introduced by Williams (1992), and that later developed into the actor–critic and policy-gradient algorithms that we treat later in this book. Our development here was influenced by that by Balara- man Ravindran. Further discussion of the choice of baseline is provided there and by Greensmith, Bartlett, and Baxter (2001, 2004) and Dick (2015).
The term softmax for the action selection rule (2.9) is due to Bridle (1990). This rule appears to have been first proposed by Luce (1959).
2.8 The term associative search and the corresponding problem were introduced by Barto, Sutton, and Brouwer (1981). The term associative reinforcement learning has also been used for associative search (Barto and Anandan, 1985), but we prefer to reserve that term as a synonym for the full reinforcement learning problem (as in Sutton, 1984). (And, as we noted, the modern litera- ture also uses the term “contextual bandits” for this problem.) We note that
46 CHAPTER 2. MULTI-ARM BANDITS
Thorndike’s Law of Effect (quoted in Chapter 1) describes associative search by referring to the formation of associative links between situations (states) and actions. According to the terminology of operant, or instrumental, con- ditioning (e.g., Skinner, 1938), a discriminative stimulus is a stimulus that signals the presence of a particular reinforcement contingency. In our terms, different discriminative stimuli correspond to different states.
2.9 The Gittins index approach is due to Gittins and Jones (1974). Duff (1995) showed how it is possible to learn Gittins indices for bandit problems through reinforcement learning. Bellman (1956) was the first to show how dynamic programming could be used to compute the optimal balance between explo- ration and exploitation within a Bayesian formulation of the problem. The survey by Kumar (1985) provides a good discussion of Bayesian and non- Bayesian approaches to these problems. The term information state comes from the literature on partially observable MDPs; see, e.g., Lovejoy (1991).
